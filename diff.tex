%\documentclass[reprint,amsmath,amssymb,superscriptaddress,prl,showpacs,onecolumn]{revtex4-1}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL main_old.tex   Fri Feb  5 11:02:29 2021
%DIF ADD main.tex       Fri Feb  5 11:04:20 2021
\documentclass[preprint,amsmath,amssymb,superscriptaddress,showpacs,pre]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{color}
\usepackage{epstopdf} 
\usepackage{amsmath,amsfonts,amssymb,amsthm}
% \usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{braket}
\usepackage[sans]{dsfont}
\usepackage{mathbbol}
\usepackage{bbm}
\usepackage{subfigure}

%% Added by Pierre
\usepackage{lineno}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography



\linenumbers

\DeclareMathOperator\atanh{arctanh}
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}
%\newcommand{\bra}[1]{\ensuremath{\left< #1\right|}}
%\newcommand{\ket}[1]{\ensuremath{\left|#1\right>}}
\newcommand{\ovl}[2]{\ensuremath{\left\langle #1\middle|#2\right\rangle}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{cf.}
\newcommand{\cp}{\emph{cp.}}
%\newcommand{\braket}[2]{\left< #1,#2\right>}
%%\newcommand{\<}{\langle}
%%\renewcommand{\>}{\rangle}
\providecommand{\abs}[1]{\ensuremath{\left|#1\right|}}
\providecommand{\norm}[1]{||#1||}
\newcommand{\arctanh}[1]{\mathrm{arctanh}#1}
\newcommand{\Sign}[1]{\mathrm{Sign}#1}

\def\<{\langle}
\def\>{\rangle}
\def\vx{\vec x}
\def\vh{\vec h}

\newcommand{\ud}{\mathrm{d}}
\newcommand{\etal}{\textit{ et. al. }}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\B}{\beta}
\def\matF{{\mathbb F}}
\def\Jij{J_{ij}}

\def\Imat{\mathbb{I}}
\def\ext{\qopname\relax m{ext}}

\newcommand{\mi}{\mathrm{i}}
\DeclareMathOperator{\Tr}{Tr}

%%%% 

\newcommand{\ddroit}{\textrm{d}}
\newcommand{\Ptot}{P(A_1\ldots A_L)}
\newcommand{\xz}{\vec{x}_0}
\newcommand{\xo}{\vec{x}_1}
\newcommand{\xt}{\vec{x}_2}
\newcommand{\Lam}{\bm{\Lambda}}
\newcommand{\Sig}{\bm{\Sigma}}
\newcommand{\Sigmone}{\bm{\Sigma^{-1}}}
\def\fonetwo{\frac 1 2}
\def\Cmone{C^{-1}}
\def\Lambtwo{\Lambda^2} 
\def\tildex{\tilde{x}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Xk}[1]{X^{\{#1\}}}
\newcommand{\xk}[1]{\vec{x}_{#1}}

% Pierre
\newcommand{\curlynormalpar}[1]{\exp\left\{-\frac{1}{2}\left( #1 \right)\right\}}
\newcommand{\curlynormal}[1]{\exp\left\{-\frac{1}{2} #1 \right\}}
\newcommand{\iSig}{\bm{\Sigma^{-1}}}
\newcommand{\iC}{\bm{C}^{-1}}
\newcommand{\av}[1]{\left\langle #1 \right\rangle}
\newcommand{\vsa}{\vec{s}_a}
\newcommand{\vuka}{\vec{u}_{ka}}

\newcommand{\sref}[1]{S\ref{#1}}
\newcommand{\pierre}[1]{{\color{red}Pierre: #1}}
\newcommand{\martin}[1]{{\color{blue}Martin: #1}}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\title{Global multivariate model learning from hierarchically correlated data}
% \date{}

\author{Edwin Rodriguez Horta} 
\affiliation{Sorbonne Universit\'{e}, CNRS, Institut de Biologie
  Paris-Seine, Laboratoire de Biologie Computationnelle et
  Quantitative -- LCQB, Paris, France}
\affiliation{Group of Complex Systems and Statistical Physics, Department of Theoretical Physics, University of Havana, Havana, Cuba}
%
\author{Alejandro Lage} 
\affiliation{Group of Complex Systems and Statistical Physics, Department of Theoretical Physics, University of Havana, Havana, Cuba}
%
\author{Martin Weigt} 
\affiliation{Sorbonne Universit\'{e}, CNRS, Institut de Biologie
  Paris-Seine, Laboratoire de Biologie Computationnelle et
  Quantitative -- LCQB, Paris, France}
% 

\author{Pierre Barrat-Charlaix} 
\email{Correspondence to: Pierre Barrat-Charlaix, \bf{pierre.barrat@unibas.ch}}
\affiliation{Biozentrum, Universit\"at Basel, Basel, Switzerland}




\begin{abstract}
The \DIFdelbegin \DIFdel{Inverse problem of Statistical Physics }\DIFdelend \DIFaddbegin \DIFadd{inverse statistical physics }\DIFaddend infers maximum-entropy models \DIFdelbegin \DIFdel{compatibles with a corresponding }\DIFdelend \DIFaddbegin \DIFadd{compatible with a }\DIFaddend set of empirical averages \DIFdelbegin \DIFdel{extracted }\DIFdelend \DIFaddbegin \DIFadd{estimated }\DIFaddend from a high-dimensional dataset of \DIFaddbegin \DIFadd{independently distributed }\DIFaddend equilibrium configurations. \DIFdelbegin \DIFdel{Practical interest extended these methods to non-equilibrium data  like  time series, where detailed balance does not hold. }\DIFdelend However, in several applications, data \DIFdelbegin \DIFdel{samples  result from evolutionary processes }\DIFdelend \DIFaddbegin \DIFadd{result from stochastic evolutionary processes, }\DIFaddend and the relation between configurations is \DIFdelbegin \DIFdel{conditioned }\DIFdelend \DIFaddbegin \DIFadd{characterized }\DIFaddend by a hierarchical \DIFdelbegin \DIFdel{structure of the population. This makes sample statistics a superposition of  signals coming from both: internal configuration correlations and relatedness correlations produced by common evolutionary  history}\DIFdelend \DIFaddbegin \DIFadd{correlation structure}\DIFaddend , typically represented by a tree. \DIFdelbegin \DIFdel{How to disentangle both sources of correlation for a better parameter inference is an open question to explore. Here we propose an approach  where the evolutive process through a phylogenetic tree is }\DIFdelend \DIFaddbegin \DIFadd{In turn, empirical averages of observables superpose intrinsic signals related to the equilibrium distribution of the studied system, and spurious historical (or phylogenetic) signals characterizing the specific correlated data-generating process. The naive application of maximum-entropy techniques therefore leads to systematic biases and an effective reduction of the sample size. To advance on the currently open task of extracting intrinsic signals from correlated data, we study a system }\DIFaddend described by a multivariate Ornstein-Uhlenbeck \DIFdelbegin \DIFdel{dynamics characterized by  gaussian propagator and stationary distributions. Inference of these distributions from phylogenetic non-independent samples is solved in a Bayesian framework. This procedure can be extended to discrete variables by using a binary representation of data and approximating  binary sates by continuous variables. Our method proposes a possible way for a better estimation of both, equilibrium and dynamic parameters, making  applications more accurate. }\DIFdelend \DIFaddbegin \DIFadd{process defined on a finite tree. Using a Bayesian framework, we can disentangle covariances in the data, which are result of their multivariate Gaussian equilibrium distribution, from those resulting from the historical correlations. Our approach leads to a clear gain in accuracy in the inferred equilibrium distribution, which corresponds to an effective two- to fourfold increase in sample size.
}\DIFaddend \end{abstract}


\maketitle


\section{Introduction}
\label{sec:int}

\DIFaddbegin \DIFadd{With the emergence of large, high-dimensional datasets for complex systems across disciplines, methods of }{\it \DIFadd{inverse statistical physics}} \DIFadd{have seen rapidly growing interest during the last years. In the most standard setting , the data provide observational samples of the "microscopic" degrees of freedom of the system under study -- this can be biological sequences, firing patterns of neurons, individuals in animal or human groups, stock prices etc. Within a static modeling approach, frequently based on the maximum-entropy approach, data $\vec x$ are assumed to be generated independently from some unknown probability distribution $P(\vec x)$. This distribution describes the underlying interaction patterns between the observed degrees of freedom, and has to be learned from data to unveil the rules governing the system. In some, more rare cases, data correspond to observed time series , but the theoretical and algorithmic development is much less advanced than in the case of independent static data, assuming, e.g., that measurements are taken at the microscopic time scale.
}

\DIFadd{One of the biggest application areas of inverse statistical mechanics is the modeling of biological sequences, and we will use them to motivate our otherwise theoretical-methodological study. Typical datasets are so called homologous protein families, collecting ensembles of proteins of common evolutionary ancestry. Despite their strong divergence in amino-acid sequences, the proteins in such families have highly similar biological functions and three-dimensional fold structures. The different sequences can thereby, in a statistical-physics perspective, be seen as microscopically distinct solutions for the same macroscopic behavior.
}

\DIFadd{In such protein families, which are typically represented by large multiple-sequence alignments, at least two complementary types of biological information are contained:
}\begin{itemize}
    \item {\it \DIFadd{Phylogenetic information:}} \DIFadd{the distances between sequences carry information about the evolutionary time since the common ancestor. Using phylogenetic methods we may reconstruct the evolutionary history of a protein family (and of the species carrying the proteins), and also the sequences of the ancestral proteins. 
    }\item {\it \DIFadd{Coevolutionary information}} \DIFadd{considers the correlated evolution of positions in proteins, frequently related to contacts in the folded protein, even for positions distant along the sequence. This information has been massively used in protein-structure prediction, most recently in combination with structurally supervised deep learning, but also to infer mutational landscapes or networks of interacting proteins.
}\end{itemize}

\DIFadd{These two types of information are contained in two complementary features of the data set: phylogenetic inference is based on the comparative analysis of the rows of the best, i.e., of entire proteins; and coevolutionary inference is based on the comparative analysis of MSA columns, which describe the amino-acid usage of the individual positions across proteins.
}

\DIFadd{For methodological reasons, each type of inference assumes the absence of the other information: phylogenetic inference assumes independent evolution of all positions; coevolutionary inference assumes that proteins form an independently and identically distributed sample of the model to be inferred.
}

\DIFadd{It has turned out to be hard to combine the two types of information in a single approach, even if the existence of, e.g, phylogenetic correlations between sequences influences the results of coevolutionary analysis. Here we propose to advance on the question of phylogeny-aware coevolutionary inference using the simplest nontrivial model setting: correlated multivariate Ornstein-Uhlenbeck (OU) processes.
}

\DIFaddend In the last decades, biology has seen impressive progress in experimental techniques which has resulted in a large increase of available data. 
This is especially visible in the case of biological sequences,  with databases now harboring a vast amount of high-quality DNA or protein sequences \cite{sayers_genbank_2019,uniprotconsortium_uniprot_2018}. 
This has in turn fuelled a development of quantitative methods to model forces guiding the evolution of sequences or other biological traits [REFS/EXAMPLES?]. 
A common idea in this context is that it is possible to use characteristics of homologous genes or organisms to construct models of the selection acting on them. 
A successful example in this regard is the representation of protein sequences by probabilistic models in the so-called DCA method \cite{levy_potts_2017,cocco_inverse_2018}. 

Models built in this way usually assume that biological characteristics of related organisms are evolving under the same constraints but are independent from each other in the statistical sense. 
This allows simple formulation for model learning. 
However, the phylogenetic relations between evolving entities are in direct contradiction with this simplifying hypothesis, and ignoring them can result in biases when learning models. 
For instance, it has been shown that phylogenetic relations between protein sequences induce non-trivial correlations that are not related to protein function \cite{qin_power_2018}. 

In order to correct for these biases, it is necessary to disentangle the effects due to selection forces and those due to phylogeny. 
However, this becomes a hard problem when dealing with complex models where traits under study are not independent from each other. 
Accounting for these biases then requires one to understand how interdependent traits under selection evolve along a tree. 
A historically well-known way to represent such processes is to use Ornstein-Uhlenbeck dynamics (OU), which models traits as a Gaussian vector evolving in a quadratic potential that represents selection forces \cite{uhlenbeck_theory_1930, felsenstein_phylogenies_1988, hansen_stabilizing_1997}.
This method is commonly used in the field of phylogenetic comparative methods (PCM) \cite{bartoszek_phylogenetic_2012,mitov_fast_2020}. 

This modeling approach is \emph{a priori} limited to continuous traits, but could potentially be used for protein sequences combined with a continuous-variable approximation, that has successfully been used in the past \cite{jones_psicov_2012,barton_large_2014,baldassi_fast_2014}. 
In this context, the equilibrium distribution reached by the OU process represents the probability distribution given by the DCA method, which can be used to predict non-trivial structural contacts in the protein fold, effects of amino-acid mutations or even designing novel functional sequences \cite{morcos_direct-coupling_2011, figliuzzi_coevolutionary_2016, russ_evolution-based_2020}. 


In this work, we are interested in constructing an inference method for parameters of an OU process from data correlated through a tree.
Our approach is purely methodological, and the data can represent any set of continuous phenotypic traits, \emph{e.g.} from different organisms, with the tree indicating the phylogenetic relations between data points. 
Inferred parameters then represent the selection forces without biasing effects from the phylogeny. 
The manuscript is divided as follows: we first review in section \ref{sec:ornstein_uhlenbeck_dynamics} the main characteristics of the multivariate OU process. 
We then describe the setting of the inference problem that we want to solve in section \ref{sub:statement_of_the_problem}, propose a solution in sections \ref{sub:Calculation_of_the_likelihood} and \ref{sub:maximization_of_the_likelihood}.
Finally, we present results obtained on simulated data in section \ref{sec:Results}, with the context of pairwise models of protein sequences in mind. 


\section{The multivariate Ornstein-Uhlenbeck process}
\label{sec:ornstein_uhlenbeck_dynamics}

We consider a system characterized by $L$ continuous degrees of freedom and whose state is fully described by an $L$-dimensional vector $\vx\in\mathbb{R}^L$. These degrees of freedom can be continuous phenotypic traits of some living organism, or the sequence of a gene or a protein if a continuous approximation is made. 
At equilibrium, $\vx$ is assumed to be normally distributed,
\begin{equation}
	P_{eq}(\vx) = \frac{1}{Z(\bm{J})}\curlynormal{\vx^T\bm{J}\vx}\ ,
	\label{eq:eq_distribution}
\end{equation}
where $\bm{J}$ is the symmetric, positive definite \emph{coupling matrix} and $Z(\bm{J}) = \sqrt{(2\pi)^L/\det \bm{J}}$ is the normalization constant; the means of all components of $\vx$ are set to zero without loss of generality. 
We are interested in inferring the coupling matrix from a given amount of observed states $\vx$ of the system.
If these observations were independent from each other, due to the simple Gaussian form of Eq.~\eqref{eq:eq_distribution}, $\bm{J}$ would simply be equal to the inverse of the empirical \emph{covariance matrix} of the data, written $\bm{C}=\bm{J}^{-1}$. 

However, we consider the case where observations are not independent. 
On the contrary, they result from a dynamical process taking place during a finite amount of time, and different data-points are therefore correlated to each other.
This dynamical process is described below.

We suppose that the considered system evolves according to the following Langevin equation 
\begin{equation}
	\gamma^{-1}\frac{\text{d}\vx}{\text{d}t} = - \bm{J}\vx + \vec{\xi}(t).
	\label{eq:langevin}
\end{equation}
Here, $\vec{\xi}(t)$ is a vector of uncorrelated white noise, and $\gamma^{-1}$ is the characteristic timescale governing the dynamics. 
In short, Eq.~\eqref{eq:langevin} states that the system described by $\vx$ undergoes Brownian motion in a quadratic energy landscape characterized by the coupling matrix $\bm{J}$. 

We are not interested in $\vx$ directly, but rather in its probability distribution $P(\vx\vert\,\vx_0,\Delta t)$, \ie~in the probability to find the system in state $\vx$ knowing it was in state $\vx_0$ some time $\Delta t$ in the past. 
The Fokker-Planck equation corresponding to Eq.~\eqref{eq:langevin} is straightforward to write, 
\begin{equation}
	\gamma^{-1} \partial_t P = \left(-\sum_{a,b=1}^L\frac{\partial}{\partial x_a} J_{ab}x_b + \sum_{a=1}^L\frac{\partial^2}{\partial x_a^2}\right) P, 
	\label{eq:FP}
\end{equation}
where the parenthesized expression on the right hand side is understood as an operator acting on $P$. 
The solution to Eq.~\eqref{eq:FP} is a multivariate normal distribution~\cite{singh2017multiOU}:
\begin{equation}
	P(\vx|\, \vx_0, \Delta t) = \left[(2\pi)^N\det\Sig\right]^{-1/2}
	\curlynormal{(\vx-\vec{\mu})^T\iSig(\vx- \vec{\mu})},
	\label{eq:OUpropagator}
\end{equation}
where we introduce the matrices $\Sig$ and $\Lam$ as well as the vector $\vec{\mu}$ as 
\begin{equation}
	\Lam = e^{-\gamma\bm{J}}, \qquad \vec{\mu} = \Lam^{\Delta t} \vx_0, \qquad \Sig = \bm{J}^{-1}(\mathbb{1} - \Lam^{2 \Delta t}).
	\label{eq:def_lambda}
\end{equation}
Eqs.~\eqref{eq:OUpropagator} and~\eqref{eq:def_lambda} define a multivariate \emph{Ornstein-Uhlenbeck} (OU) process. 

Note that since matrix $\Lam$ is an exponential of $\bm{J}$, it is symmetric, has strictly positive eigenvalues and commutes with $\bm{J}$. 
We also underline that $\Sig$ and $\vec{\mu}$ depend on $\Delta t$, although this dependence is not explicitly written in our notation to make it less heavy. 
By taking $\gamma\Delta t \gg 1$ and using the fact that $\bm{J}$ has strictly positive eigenvalues, one immediately recovers Eq.~\eqref{eq:eq_distribution}, meaning that the OU process converges to the desired equilibrium distribution.

We can compute the joint distribution of two configurations $\vx_1$ and $\vx_2$ separated by a time $\Delta t$ by multiplying Eqs.~\eqref{eq:eq_distribution} and~\eqref{eq:OUpropagator}, 
\begin{eqnarray}
    P(\vx_1, \vx_2\vert\,\Delta t) &= & P(\vx_1 |\, \vx_2,\Delta t) \times P_{eq}(\vx_2)
    \nonumber\\
    &\propto& \curlynormalpar{\vx_1^T\iSig\vx_1 + \vx_2^T\iSig\vx_2 - 2\vx_1^T\Lam^{\Delta t}\iSig\vx_2}. 
	\label{eq:OUjoint}	
\end{eqnarray}
This equation illustrates the \emph{time reversibility} of the OU process. 
Indeed, the distribution is symmetric in $\vx_1$ or $\vx_2$ and does not depend on which configuration came first.

Equation \eqref{eq:OUjoint} allows for computing the joint covariance of the correlated equilibrium configurations $\vx_1$ and $\vx_2$. 
The probability distribution in Eq.~\eqref{eq:OUjoint} is normal with an inverse covariance matrix defined by blocks: $\Sig$ on the diagonal and $-\Lam^{\Delta t}\Sig$ off-diagonal. 
By inverting this block matrix, given that $\Lam$ and $\Sig$ commute and are invertible, one obtains the following covariance: 
\begin{equation}
	\langle \vx_1\vx_2^T \rangle_{\Delta t} = \Lam^{\Delta t}\bm{J}^{-1} = \Lam^{\Delta t}\bm{C}. 
	\label{eq:pairwisecov}
\end{equation}


Eq.~\eqref{eq:pairwisecov} allows us to readily distinguish two regimes. 
Let us call $\rho_a$ the eigenvalues of $\bm{J}$. 
The eigenvalues of $\Lam^{\Delta t}\bm{C}$ are then equal to $\rho_a^{-1}e^{-\gamma\rho_a\Delta t}$. 
Since all $\rho_a$ are positive, the eigenvalues of $\Lam^{\Delta t}\bm{C}$ vanish exponentially over time. The slowest timescale of exponential decay is set by $\tau_c^{-1} = \gamma\rho_{min}$, with $\rho_{min}$ being the smallest eigenvalue of $\bm{J}$. 
Thus, for $\Delta t / \tau_c \gg 1$, $\vx_1$ and $\vx_2$ are uncorrelated. 
If this is verified for all pairs of observations $\vx_i$ and $\vx_j$, the regime is that of \emph{uncorrelated} data -- the inference of $\bm{J}$ can simply be performed by inverting the empirical covariance matrix extracted from the data. 
Inversely, for $\Delta t / \tau_c \ll 1$, $\vx_1$ and $\vx_2$ are highly correlated, defining a \emph{strongly correlated} regime. 
It should be noted that for $\Delta t = 0$, the joint correlation matrix of $\vx_1$ and $\vx_2$ becomes non invertible, and Eq.~\eqref{eq:pairwisecov} becomes irrelevant. Actually, $\vx_1$ and $\vx_2$ coincide at that point, \ie~we have $P(\vx_1,\vx_2|\,\Delta t=0) = P_{eq}(\vx_1) \times \delta(\vx_1 - \vx_2)$ using the $L$-dimensional Dirac distribution.






\section{Methods}
\label{sec:methods}

\subsection{Statement of the problem}
\label{sub:statement_of_the_problem}


\begin{figure}[!htb]
	\includegraphics[width=0.8\textwidth]{Figures/tree.pdf}
	\caption{Schematic representation of a tree $\mathcal{T}$ underlying the data generating process. The process starts at the root node $r$ with a configuration $\vx_r$ sampled from $P_{eq}(\vx_r)$. The dynamics consist in independent realizations of the OU process on all branches from ancestral nodes $a(n)$ to child nodes $n$ over times corresponding to the branch length $\Delta t_{n,a(n)}$, initialized in the ancestral configuration $\vx_{a(n)}$. The observable data only consist of configurations of the leaf nodes (grey circles in the figure), while configurations of ancestral nodes remain unknown. There are no restrictions on the topology of tree $\mathcal{T}$ and the length of the branches.}
	\label{fig:sample_tree}
\end{figure}


The problem discussed here is the inference of the probability distribution describing samples that are hierarchically correlated by a tree, cf.~Fig.~\ref{fig:sample_tree}. 
Formally, we assume that the data consists of $N$ real-valued vectors of length $L$, denoted $\{\vx_i\}\in\mathbb{R}^L$ with $i=1,...,N$. 
Taken individually, we assume that the $\vx_i$ are distributed according to Eq.~\eqref{eq:eq_distribution}, \ie~according to a multivariate Gaussian of zero mean and covariance $\bm C$.
By construction, the equilibrium covariance between any pair of elements of a given vector $\vx=(x^1,...,x^L)$ is given by the inverse of the coupling matrix: $\langle x^a x^b\rangle-\langle x^a\rangle\langle x^b\rangle = \bm C_{ab} = (\bm J^{-1})_{ab}$ for all $a,b=1,...,L$. 
This implies that inferring the coupling matrix defining the probability distribution amounts to finding the \emph{equilibrium} covariance matrix $\mathbf{C}$.  

However, this covariance cannot be directly measured as we consider observations that are not independently distributed.
Instead, the set of measured configurations $\{\vx_i\}_{i=1,...,N}$ is the result of an Ornstein-Uhlenbeck (OU) process taking place on a tree $\mathcal{T}$, as is illustrated in Fig.~\ref{fig:sample_tree}:
\begin{itemize}
    \item The process starts at the root node $r$ with a state vector $\vx_r$ drawn from the equilibrium distribution $P_{eq}$.
    \item On each branch $(n,a(n))$ of length $\Delta t_{n,a(n)}$ connecting node $n$ with its ancestral node $a(n)$, the dynamics follow Eq.~\eqref{eq:langevin}, starting from initial condition $\vx_{a(n)}$, and running for time $\Delta t_{n,a(n)}$. In other words, given the state $\vx_{a(n)}$ of the ancestral node, $\vx_n$ is sampled from $P(\vx_n |\, \vx_{a(n)}, \Delta t_{n,a(n)})$, see Eq.~\eqref{eq:OUpropagator}
    \item As a consequence, OU processes on branches stemming from common ancestral node evolve independently, but from an identical initial condition.
    \item Observed data vectors correspond to the states of the leaves of the tree at the end of this process. The states of the internal nodes are not part of the observed data and remain unknown. 
\end{itemize}
This process is thought to represent the evolution of biologic traits along a phylogenetic tree, with the leave nodes corresponding to traits observed in today's species. Note that due to the reversible nature of our OU process, the joint probability of any pair of leaf configurations $\vx_i$ and $\vx_j$, with $i,j\in\{1,...,N\}$, is given by  $P(\vx_i,\vx_j|\,\Delta t_{ij})$ (Eq.~\eqref{eq:OUjoint}), with $\Delta t_{ij}$ denoting the total branch length of the path connecting $i$ and $j$ in the tree.

The OU process is characterized by the quadratic potential $\bm J = \bm C^{-1}$ and the rate $\gamma$. 
Hence, the joint statistics of the leaf configurations $\{\vx_i\}_{i=1,...,N}$ (\ie~the data) is fully determined by $\bm C$, $\gamma$, and the tree $\mathcal{T}$. 
The aim of this work is to derive a method for inferring the most likely values of $\bm C$ and $\gamma$ given the knowledge of the data $\mathcal{D}=\{\vx_i\}_{i=1,...,N}$ and the underlying tree $\mathcal{T}$. We consider here that both the topology and the branch lengths of $\mathcal{T}$ are known. 

This problem shows two notable extreme cases: 
The first one is the case where the typical branch length of the tree is short compared to the timescales of the OU process.
As a consequence, leaf configurations are close to identical to the root, \ie~$\vx_i\simeq\vx_r$, and the inference of $\bm C$ becomes impossible. 
The second one is the opposite case where the typical branch length of the tree is long compared to the longest timescale of the OU process $\tau_c$.
In this case, the configuration of a child node is close to independent from that of its ancestor, and leaf configurations can be considered as independent samples from the equilibrium distribution $P_{eq}$.
$\bm C$ can then be readily estimated by computing the empirical covariance matrix. 
We are interested here in the intermediate regime where substantial tree-mediated correlations between data
make it impossible to simply estimate $\bm C$ with the empirical covariance, but the depth of the tree introduces enough variability in the data for one to hope of reconstructing the energy potential $\bm{J}$.

We adopt a Bayesian inference approach by writing the probability of a given set of parameters $\{\bm C, \gamma\}$ given the data $\{\mathcal{D},\mathcal{T}\}$ using Bayes' equation
\begin{equation}
	P(\bm C, \gamma\vert\mathcal{D},\mathcal{T}) \propto P(\mathcal{D}\vert\bm C, \gamma,\mathcal{T})\cdot P(\bm C, \gamma),
	\label{eq:bayesian_likelihood}
\end{equation}
with the proportionality constant not depending on the parameters $\{\bm C, \gamma\}$. Here, $P(\bm C, \gamma)$ can be any arbitrarily chosen prior distribution. The difficulty in Eq.~\eqref{eq:bayesian_likelihood} lies in the estimation of the likelihood $P(\mathcal{D}\vert\bm C, \gamma, \mathcal{T})$,
\ie~of the joint probability of the datapoints $\mathcal{D}=\{\vx_i\}_{i=1,...,N}$ for an OU process given by its parameters $\{\bm C, \gamma\}$ and the tree $\mathcal{T}$.  We detail the computation of this probability in the following section. 

\subsection{Calculation of the likelihood} % (fold)
\label{sub:Calculation_of_the_likelihood}

The joint distribution of two configurations $\vx_1$ and $\vx_2$ separated by time $\Delta t$ is given by Eq.~\eqref{eq:OUjoint} and corresponds to a joint normal distribution. 
This means that the vector $\vec{X}=[\vx_1, \vx_2]$, \emph{i.e.} the concatenation of vectors $\vx_1$ and $\vx_2$, follows a normal distribution with zero mean and variance described above in Eqs.~\eqref{eq:def_lambda}.
Of importance here is that this property of the OU process can be extended to the joint distribution of any subset of nodes in a tree. 
In other words, if we now define $\vec{X}=[\vx_1,\ldots,\vx_N]$ to be the concatenation of all configurations in our dataset $\mathcal D$, we can write the distribution of $\vec{X}$ as
\begin{equation}
	P(\vec{X}\vert\,\bm{C},\gamma,\mathcal{T}) = \left((2\pi)^{LN}\det\mathbb{G}\right)^{-\frac{1}{2}}\curlynormal{\vec{X}^T\mathbb{G}^{-1}\vec{X}},
	\label{eq:multiOU_jointall}
\end{equation}
where $\mathbb{G}$ is the \emph{joint covariance matrix} and depends on the tree as well as on $\bm{C}$ and $\gamma$.

The joint covariance matrix is a matrix of dimension $(L\cdot N)\times(L\cdot N)$, built by $N\times N$ blocks of size $L\times L$ with entries
\begin{equation}
	\mathbb{G}_{ij}(a,b) = \av{x_i^a x_j^b} - \av{x_i^a}\av{x_j^b},\ \ \ \ i,j\in\{1,...,N\}; a,b\in\{1,...,L\},
	\label{eq:jointcov_1}
\end{equation}
where the (zero) marginals $\av{x_i^a}$ and $\av{x_j^b}$ are explicitly written for clarity.
Each block $\mathbb{G}_{ij}$ is describing the connected correlations between two data vectors $\vx_i$ and $\vx_j$, which are separated by time $\Delta t_{ij}$, resulting as the sum of all branch lengths of the path connecting $i$ and $j$ on tree $\mathcal{T}$. Because the OU process is time reversible, we can directly apply Eq.~\eqref{eq:pairwisecov} and give all blocks of $\mathbb{G}$ in closed form, 
\begin{equation}
	\mathbb{G}_{ij} =
	\begin{cases}
		\bm C & \text{if $i=j$}\\
		\Lam^{\Delta t_{ij}}{\bm C} & \text{otherwise}  ,
	\end{cases}
	\label{eq:jointcov_2}
\end{equation}
using the (currently unknown) covariance matrix $\bm C$ of a single equilibrium vector $\vx$. We remind here that $\Lam = e^{-\gamma\iC}$ depends only on $\gamma$ and $\bm C$, and commutes with $\bm C$. As a direct consequence, all blocks $\mathbb{G}_{ij}$ commute with each other and with $\bm C$.

Eq.~\eqref{eq:multiOU_jointall} allows us to compute the log-likelihood of the data $\vec{X}$ as a function of $\vec{X}$ itself and of the joint covariance matrix. Indeed, taking its logarithm immediately gives
\begin{equation}
	\mathcal{L}_{\mathcal{D}}(\mathbb{G}) = -\frac 1 2 \log \det  \mathbb{G}  -\frac 1 2 \vec{X}^T\mathbb{G}^{-1} \vec{X} + \text{const}\ ,
	\label{eq:likelihood_2}
\end{equation}
but this expression is impractical for any numerical evaluation due to the large dimension of $\mathbb{G}$. However, the particular block structure of $\mathbb{G}$ described in Eq.~\eqref{eq:jointcov_2} allows us to simplify the expression. To do so, we first introduce the eigenvalues and eigenvectors 
$\left\{\rho_a, \vsa\right\}$ of $\iC$, where the index $a$ runs from $1$ to $L$ and vectors $\vsa$ are of dimension $L$. By definition, we have $\rho_a>0$ for all $a$.  Using now Eq.~\eqref{eq:jointcov_2}, we immediately see that the vectors $\vsa$ are also eigenvectors of the individual blocks $\mathbb{G}_{ij}$ with eigenvalues $z(\rho_a, \Delta t_{ij})$ where we introduced
\begin{equation}
	z(\rho_a, \Delta t_{ij}) = \rho_a^{-1}e^{-\gamma\rho_a\Delta t_{ij}} \ .
	\label{eq:z}
\end{equation}
By convention, $\Delta t_{ii} = 0$ and the diagonal blocks are thus included via $z(\rho_a, \Delta t_{ii}) = \rho_a^{-1}$.


As the next step, we introduce $N\times N$-dimensional matrices ${\bm G}^a, a=1,...,L,$ with elements 
\begin{equation}
	{\bm G}^a_{ij} = z(\rho_a, \Delta t_{ij})\ ,\ \ \ \ 1\leq i,j\leq N\ .
	\label{eq:subG_def}
\end{equation}
In other words, for a given index $1\leq a\leq L$, ${\bm G}^a$ is the matrix built by replacing all blocks of $\mathbb{G}$ by their respective $a$th eigenvalue.
Matrices ${\bm G}^a$ are symmetric and have their own eigenmodes, that we denote by $\left\{\lambda_{ka}, \vuka\right\}_{k=1,...,N}$.

To obtain the eigenmodes of the joint covariance matrix $\mathbb{G}$ as a function of the $\vsa$ and $\vuka$, we construct the direct product of vectors $\vsa$ and $\vuka$, defining vectors $\vec{S}_{ka}$ of dimension $L\times N$:
\begin{equation}
	\begin{split}
		\vec{S}_{ka} &= \vuka \otimes \vsa \\
		&= [u_{ka}^1\cdot\vsa, \ldots, u_{ka}^N\cdot\vsa].
		% &= [u_{ka}(1)s_a(1), \ldots, u_{ka}(1)s_a(L); \ldots; u_{ka}(N)s_a(1), \ldots, u_{ka}(N)s_a(L)]
	\end{split}
	\label{eq:Sdef}
\end{equation}
The $i$th block vector of $\vec{S}_{ka}$ will thus be written as $\vec{S}_{ka}^i = u_{ka}^i\cdot\vsa$. 
We can now show that $\vec{S}_{ka}$ are eigenvectors of matrix $\mathbb{G}$ by considering the $i$th block vector of the product $\mathbb{G}\cdot\vec{S}_{ka}$:
\begin{equation}
	\begin{split}
		\left(\mathbb{G}\cdot\vec{S}_{ka}\right)^i &= \sum_{j=1}^N \mathbb{G}_{ij} u_{ka}^j\cdot\vsa \\
		&= \sum_{j=1}^N z(\rho_a, \Delta t_{ij}) u_{ka}^j\cdot \vsa \\
		&= (\mathbf{G}^a\cdot\vuka)^i \cdot \vsa\\
		&= \lambda_{ka}(u_{ka}^i\cdot\vsa) \\
		&= \lambda_{ka}\vec{S}_{ka}^i \ .
	\end{split}
\end{equation}
We have first used the fact that $\vsa$ is an eigenvector of $\mathbb{G}_{ij}$, then the definition of ${\bm G}^a$, and finally the fact that $\vuka$ is an eigenvector of ${\bm G}^a$. 
This demonstrates that the eigenmodes of $\mathbb{G}$ are $\left\{ \lambda_{ka}, \vec{S}_{ka} \right\}$ with $1\leq k\leq N$ and $1\leq a \leq L$. 
Since $\mathbb{G}$ is the covariance matrix of a Gaussian distribution, we conclude the $\lambda_{ka}$ to be strictly positive.

Note that this decomposition of the eigenvectors leads to a drastic decrease in computational complexity for diagonalizing $\mathbb{G}$ (at given $\bm C$, $\gamma$ and $\cal T$), and in consequence also for calculating the likelihood according to Eq.~\eqref{eq:likelihood_2}, which depends on the inverse covariance matrix $\mathbb{G}^{-1}$. Matrix $\mathbb{G}$ has linear dimension $LN$, so the numerical diagonalization or inversion takes time ${\cal O}( (L N)^3)$. This is hardly achievable for systems of realistic length $L$ of the state vector, and sufficient number $N$ of data points for model learning. Following the above description, we need to first diagonlize $\bm C^{-1}$ (or equivalently $\bm C$), which requires time of ${\cal O}(L^3)$, followed by inversion of the $L$ matrices ${\bm G}^a$, each one having linear dimension $N$. The total time complexity therefore results in ${\cal O}(L^3) + {\cal O}(L\cdot N^3)$, and the calculation can be easily achieved even on a standard PC. This observation is essential for inference, since we need to redo this calculation for many realizations of $\bm C$ and $\gamma$, in order to find the ones maximizing the likelihood given the data $\cal D$ and the tree $\cal T$. As is shown in section \sref{sub:homogeneous_and_fully_balanced_tree}, this calculation simplifies even more when considering a fully balanced and homogeneous tree. In this case, the matrices $\bm G^a$ commute and can be diagonalized simultaneously and analytically for any value of $\rho^a$.

For the case of arbitrary trees, Eq.~\eqref{eq:likelihood_2} can now be rewritten using the eigen-decomposition of $\mathbb{G}$: 
\begin{equation}
	\begin{split}
		\mathcal{L}_{\mathcal{D}}(\mathbb{G}) &= -\frac 1 2 \sum_{k=1}^N\sum_{a=1}^L \log\lambda_{ka} - \frac 1 2 \sum_{k=1}^N\sum_{a=1}^L \lambda_{ka}^{-1}(\vec{X}\cdot \vec{S}_{ka})^2\\
		&= -\frac 1 2 \sum_{k,a} \left(\log\lambda_{ka} + \lambda_{ka}^{-1}	\left(\sum_{i=1}^N u_{ka}^i\vx_i\cdot\vsa\right)^2 \right).
	\end{split}
	\label{eq:likelihood_3}
\end{equation}
Eq.~\eqref{eq:likelihood_3} expresses the likelihood as a function of $\vuka$, $\lambda_{ka}$ (resulting from the tree $\cal T$ and given $\rho^a$) and $\vsa$ (resulting from $\bm C$). 
However, the definition of $\bm G^a$ in Eq.~\eqref{eq:subG_def} makes clear that its eigenmodes $\{\lambda_{ka}, \vuka \}$ depend only of the eigenvalues $\rho_a$ of $\iC$, on $\gamma$, as well as of the structure of the tree through the quantities $\Delta t_{ij}$, although this dependence cannot be analytically expressed in a simple manner. 
This means that the likelihood in equation~\eqref{eq:likelihood_3} is in fact a function of $\{\rho_a, \vsa\}$, \emph{i.e.} the eigenmodes of $\iC$, of the time scale parameter $\gamma$ and of the pairwise distances on the tree $\Delta t_{ij}$.



% subsection derivation_of_the_likelihood (end) 

\subsection{Maximizing the likelihood} % (fold)
\label{sub:maximization_of_the_likelihood}

As stated at the beginning of this section, our main task is to find the equilibrium covariance matrix $\bm C$ that maximizes the likelihood of the data. 
We also need to find the optimal time scale $\gamma$. 
In Eq.~(\eqref{eq:likelihood_3}), the likelihood is expressed as a function of $\gamma$ and $\{\rho_a, \vsa\}$, \ie~the eigenvalues and eigenvectors of $\iC$, either directly or through the quantities $\{\lambda_{ka}, \vuka \}$. 
We know attempt to maximize the likelihood with respect to the eigenmodes $\{\rho_a, \vsa\}$ and to the time scale $\gamma$. 

In order to perform this optimization, we need to compute the gradient of the likelihood with repsect to the eigenvectors $\{\vsa\}$. 
Since $\iC$ is a symmetric matrix, its eigenvectors form an orthogonal basis of the vector-space of dimension $L$ and their components cannot be changed independently.  
One possible parametrization for the $\{\vsa\}$ consists in using $L(L-1)/2$ scalar \emph{Eulerian angles} $\{\theta_{\alpha\beta}\}$ with $1\leq \alpha < \beta \leq L$~\cite{Raffenetti1970GEA, Shepard_param_OM}. 
With the $L$ eigenvalues $\rho_a$, this results in $L(L+1)/2$ independent values that fully parametrize the $L(L+1)/2$ values of $\iC$. 
A second possibility, that we have found faster in practice, is to express the matrix of the $\{\vsa\}$ as the exponential of a skew-symetric matrix, see section \ref{sub:parametrizations_of_eigenvectors} of the SM. 
However, this parametrization does not allow a simple analytical expression of the gradient of the likelihood, and we use it along with automatic differentiation \DIFdelbegin \DIFdel{. }%DIFDELCMD < \pierre{Edwin, can you add a reference to the package you used in the .bib and here?}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Zygote.jl-2018}}\hspace{0pt}%DIFAUXCMD
. 
}\DIFaddend For this reason, we use the Eulerian angles below to express the gradient of the likelihood. 

As a first step, we need to compute the gradient of the likelihood $\mathcal{L}_{\mathcal{D}}(\mathbb{G})$ with respect to all parameters $\{\rho_a, \theta_{\alpha\beta}\}$ and $\gamma$. 
To make explicit the dependences of eigenvalues and eigenvectors of the matrices $\bm G^a$ on these parameters, we introduce the notation $\vec{u}_k(\rho_a,\gamma) = \vuka$ and $\lambda_k(\rho_a,\gamma) = \lambda_{ka}$. 
Note that from the definition of $\bm G^a$ in Eq.~\eqref{eq:subG_def}, its eigenvalues and vectors depend only on the eigenvalues of $\iC$ and not on its eigenvectors. 
In the same way, we will now write $\bm G(\rho_a,\gamma)$ instead of $\bm G^a$. 

The gradient of the likelihood is obtained by differentiating Eq.~\eqref{eq:likelihood_3} with respect to the parameters of interest. 
This gives us  three equations: 
% dL / d rho
\begin{equation}
	\begin{split}
		\frac{\partial \mathcal{L}}{\partial\rho_a} &= -\frac 1 2 \sum_{k=1}^N \left\{ \frac{\partial\lambda_k}{\partial\rho_a}\lambda_k^{-1} - \frac{\partial\lambda_k}{\partial\rho_a} \lambda_k^{-2}\left(\sum_{i=1}^N u_{k}^i\vx_i\cdot\vsa\right)^2 \right. \\
		&+ \left. 2\lambda_k^{-1} \left(\sum_{i=1}^N u_k^i\vx_i\cdot\vsa\right) \left(\sum_{i=1}^N \frac{\partial u_k^i}{\partial\rho_a} \vx_i\cdot\vsa\right) \right\}, 
	\end{split}
	\label{eq:gradlikelihood_1}
\end{equation}
% dL / d theta
\begin{equation}
	\frac{\partial \mathcal{L}}{\partial\theta_{\alpha\beta}} = \sum_{k=1}^N \lambda_k^{-1} \left(\sum_{i=1}^N u_k^i\vx_i\cdot\vsa\right) \left(\sum_{i=1}^N u_k^i \vx_i\cdot\frac{\partial \vsa}{\partial\theta_{\alpha\beta}}\right),
	\label{eq:gradlikelihood_2}
\end{equation}
and
% dL / d gamma
\begin{equation}
	\begin{split}
		\frac{\partial \mathcal{L}}{\partial\gamma} &= -\frac 1 2 \sum_{k=1}^N \left\{ \frac{\partial\lambda_k}{\partial\gamma}\lambda_k^{-1} - \frac{\partial\lambda_k}{\partial\gamma} \lambda_k^{-2}\left(\sum_{i=1}^N u_{k}^i\vx_i\cdot\vsa\right)^2 \right. \\
		&+ \left. 2\lambda_k^{-1} \left(\sum_{i=1}^N u_k^i\vx_i\cdot\vsa\right) \left(\sum_{i=1}^N \frac{\partial u_k^i}{\partial\gamma} \vx_i\cdot\vsa\right) \right\}, 
	\end{split}
	\label{eq:gradlikelihood_3}
\end{equation}
The derivatives of $\vec{u}_k(\rho,\gamma)$ and $\lambda_k(\rho,\gamma)$ with respect to $\rho$ can then be computed using the following equations~\cite{matrix_cook_book}: 
\begin{equation}
	\frac{\partial\lambda_i(\rho,\gamma)}{\partial\rho} = \vec{u}_k(\rho,\gamma)^T\frac{\partial\bm{G}(\rho,\gamma)}{\partial\rho}\vec{u}_k(\rho,\gamma)
	\label{eq:grad_eigval}
\end{equation}
and
\begin{equation}
	\frac{\partial \vec{u}_k(\rho,\gamma)}{\partial\rho} = \sum_{l\neq k} \left( \vec{u}_k(\rho,\gamma)^T\frac{\partial\bm{G}(\rho,\gamma)}{\partial\rho}\vec{u}_l(\rho,\gamma) \right) \left(\lambda_k(\rho,\gamma) - \lambda_l(\rho,\gamma)\right)^{-1} \vec{u}_l(\rho,\gamma).
	\label{eq:grad_eigvec}
\end{equation}
Equivalent equations can be written for their derivatives with respect to $\gamma$. 

The computation of the gradient of $\mathcal{L}$ for a given set of parameters $\{\rho_a, \theta_{\alpha\beta}\}$ then goes as follows. 
For each eigenvalue $\rho_a$, we compute and diagonalize matrix $\bm{G}(\rho_a)$ to obtain its eigenmodes $\vec{u}_k(\rho_a)$ and $\lambda_k(\rho_a)$. 
Using equations~\eqref{eq:grad_eigval} and ~\eqref{eq:grad_eigvec} and their equivalent form for $\gamma$, we also numerically compute their derivatives with respect to $\rho_a$ and $\gamma$. 
This gives us all the quantities to estimate the gradient of $\mathcal{L}$ with respect to $\rho_a$ using equation~\eqref{eq:gradlikelihood_1}.

 The optimization is performed by a quasi-Newton method \cite{NLopt}. Details are presented in section \ref{sub:optimization_scheme} of the SM. 


% subsection maximization_of_the_likelihood (end)


\section{Results}
\label{sec:Results}
% \subsection{Choosing an appropriate time scale interval}

In order to evaluate our inference procedure, we generate artificial data corresponding to the process described in section \ref{sub:statement_of_the_problem}. 
We first build a  balanced binary tree $\mathcal{T}$ with $2^9=512$ leaves. 
The length of each branch of $\mathcal{T}$ is chosen from a uniform distribution in the interval $[0,1]$. 
We also sample positive semi-definite coupling matrix $\bm{J}$ of size $L\times L$ with $L=4$ or $L=10$, with entries normally distributed with mean $\mu_J=0.8$ and $\sigma_J=0.2$. 

In the case of statistical models of protein sequence, a major achievement is the ability of pairwise models to predict contacts in the three-dimensional structure of the protein from an inferred coupling matrix.
In order to replicate this setting and to perform interaction prediction, we randomly set to $0$ off-diagonal elements of $J$ with probability $0.7$, resulting in a \DIFdelbegin \DIFdel{coupling matrix with approximative }\DIFdelend \DIFaddbegin \DIFadd{sparsified coupling matrix of approximate }\DIFaddend density $0.3$.  
\DIFdelbegin \DIFdel{Null }\DIFdelend \DIFaddbegin \DIFadd{Zero }\DIFaddend elements of $J$ correspond to variables that do not interact, \DIFdelbegin \DIFdel{or }\DIFdelend \DIFaddbegin \DIFadd{in analogy }\DIFaddend to non-contacts in the case of an application to protein sequences.  


In order to investigate the different regimes of tree-induced correlation, we vary the parameter $\gamma$ around a reference timescale $\gamma_d$ defined as follows: 
\begin{equation}
    \gamma_d = \frac{1}{\Delta t_{av}\rho_{min}} 
\end{equation}
where $\Delta t_{av}$ is the average branch length separating two leaves of $\mathcal{T}$. 
For $\gamma \gg \gamma_d$, leaf configurations are on average well decorrelated, whereas for $\gamma \ll \gamma_d$ all leaves will be strongly correlated. 
By simulating data using different $\gamma$ in the range $[10^{-2},2]\cdot\gamma_d$, we investigate all relevant temporal regimes.  
For each value of $\gamma$, we then sample  configurations of leaves of $\mathcal{T}$ using the process described in section \ref{sub:generating_artificial_data} of the supplementary material. 
To avoid statistical noise when assessing the quality of our inference, we repeat the sampling of leaf configurations 100 times for each value of $\gamma$.

For each repetition of the sampling process, we perform our maximum likelihood procedure and obtain an inferred covariance matrix $\bm{C}_{max}$. 
As a means of comparison, we also compute the empirical covariance matrix $\bm{C}_{emp}$ as if leaf configurations were independent. 
\DIFdelbegin \DIFdel{Figure }\DIFdelend \DIFaddbegin \DIFadd{Fig.~}\DIFaddend \ref{fig:pears_L4} shows the Pearson correlation between the real covariance matrix $\bm{C}=\bm{J}^{-1}$ and the empirical or inferred ones in the $L=4$ case (similar figures for $L=10$ are in \DIFdelbegin \DIFdel{section \ref{sub:supplementary_figures}of the smallest}\DIFdelend \DIFaddbegin \DIFadd{Appendix \ref{sub:supplementary_figures}}\DIFaddend ). 
As expected, both methods perform well in the large $\gamma$ limit with a correlation close to $1$, and worse in the low $\gamma$ limit. 
In this latter case, correlations due to phylogeny are too strong for our maximum likelihood method to pick up signal, and both methods perform \DIFdelbegin \DIFdel{as }\DIFdelend \DIFaddbegin \DIFadd{equally }\DIFaddend poorly. 
However, there exists an intermediate regime where $\bm{C}_{max}$ is much closer to the actual correlation than  $\bm{C}_{emp}$. 
In \DIFdelbegin \DIFdel{figure }\DIFdelend \DIFaddbegin \DIFadd{Fig.~}\DIFaddend \ref{fig:error_1_L4}, we plot the relative $l2$-error between either covariance matrices in the left panel or coupling matrices in the right panel. 
In both cases, our maximum-likelihood method results in a consistent improvement over the empirical estimator. 
However, the relative error still reaches high values in the low $\gamma$ regime, which is likely due to $\bm{C}_{max}$ and $\bm{C}_{emp}$ being close to singular in this case. 

\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/corr_C_L4_balanced_tree_100_rep.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/corr_C_L4_balanced_tree_100_rep.pdf}
	\DIFaddendFL \caption{ Pearson correlation between empirical /maximum-likelihood   covariance matrices and the true covariance matrix. The inset plot represents the ratio between the Pearson correlation for the  maximum-likelihood covariance matrix and the one for the empirical covariance matrix. Simulations are performed for a tree of $512$ leaves  and  system size $L=4$.}
	\label{fig:pears_L4}
\end{figure}



\begin{figure*}[!htb]
	\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}{}
%DIFDELCMD < 			\centering\includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_C_L4_balanced_tree_100.pdf}
%DIFDELCMD < 		\end{subfigure}
%DIFDELCMD < 		%%%
\DIFdelFL{\hspace{1mm}
	}%DIFDELCMD < \begin{subfigure}{}
%DIFDELCMD < 			%%%
\DIFdelendFL %DIF > 	\begin{subfigure}{}
			\centering\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_J_L4_balanced_tree_100.pdf}
%DIFDELCMD < 		\end{subfigure}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/epsilon_error_L4_balanced_tree_100.pdf}
	%DIF > 	\end{subfigure}
		\DIFaddFL{\hspace{1mm}
	%DIF > 	\begin{subfigure}{}
	%DIF > 		\centering\includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_J_L4_balanced_tree_100.pdf}
	%DIF > 	\end{subfigure}
	}\DIFaddendFL \caption{\textbf{Left:}Relative $l2$-error between empirical or maximum-likelihood  covariance matrices and the true covariance matrix.  \textbf{Right:}Relative $l2$-error between empirical /maximum-likelihood  coupling matrices and the true coupling matrix. Logarithmic scale is chosen for the $y$-axis because of large values of the error at low $\gamma$. The inset in both panels show the ratio between the two errors.}
	\label{fig:error_1_L4}
\end{figure*}

An interesting way to illustrate the benefits of reconstructing the covariance matrix using knowledge of the tree is to evaluate the \DIFdelbegin \DIFdel{effective number of samples that is gained by doing so. 
Figure \sref{fig:error_vs_nseqs} shows }\DIFdelend \DIFaddbegin \DIFadd{gain in }{\em \DIFadd{effective sample size}}\DIFadd{. Intuitively, the use of correlated samples reduces the information contained in the data, as compared to an equally large dataset of }\emph{\DIFadd{i.i.d.}} \DIFadd{configurations. It is therefore interesting to compare the accuracy of our inferences with the accuracy obtained on smaller but }\emph{\DIFadd{i.i.d.}} \DIFadd{samples. To do so, we report in Fig.~\sref{fig:error_vs_nseqs} }\DIFaddend the $l2$-error between \DIFdelbegin \DIFdel{the true covariances and the }\DIFdelend \DIFaddbegin \DIFadd{true and }\DIFaddend empirical covariances computed from \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend \emph{i.i.d.} \DIFdelbegin \DIFdel{sample of size }\DIFdelend \DIFaddbegin \DIFadd{samples of variable sizes }\DIFaddend $N$. As expected\DIFdelbegin \DIFdel{the error is increasing for }\DIFdelend \DIFaddbegin \DIFadd{,  the error increases with }\DIFaddend decreasing values of $N$. 
We can use this \DIFdelbegin \DIFdel{as a calibration }\DIFdelend \DIFaddbegin \DIFadd{in turn }\DIFaddend to express values of the $l2$-error in \DIFdelbegin \DIFdel{terms of an effective sample size}\DIFdelend \DIFaddbegin \DIFadd{correlated samples in terms of effective }\emph{\DIFadd{i.i.d.}} \DIFadd{sample sizes}\DIFaddend .   
For example, the error reached by $\bm{C}_{emp}$ for $\gamma/\gamma_d\in[0.5, 1]$ and $L=4$ corresponds to the one obtained for an \emph{i.i.d.} sample of size $\sim 16$, whereas it corresponds to a sample of size $\sim 32-64$ for $\bm{C}_{max}$.
Thus, our correction is equivalent to \DIFdelbegin \DIFdel{doubling }\DIFdelend \DIFaddbegin \DIFadd{increasing by a factor 2-4 }\DIFaddend the number of effective samples.

Finally, we assess the performance of our method in improving the prediction of \DIFdelbegin \DIFdel{interactions between gaussian }\DIFdelend \DIFaddbegin \DIFadd{the network of interactions between the Gaussian }\DIFaddend variables $\{x_a\}$. 
We consider that two variables $x_a$ and $x_b$ interact if the corresponding entry in the coupling matrix is non-zero, that is $J_{ab}\neq 0$. 
\DIFdelbegin \DIFdel{We }\DIFdelend \DIFaddbegin \DIFadd{Using the data, we }\DIFaddend predict these interactions by taking the largest $n$ elements (in absolute value) of the inferred coupling matrix, resulting in $n$ predictions. 
The fraction \DIFaddbegin \DIFadd{TP$/n$ }\DIFaddend of these $n$ predictions that correspond to non-zero entries in the true matrix \DIFaddbegin \DIFadd{(TP = true positives) }\DIFaddend defines the positive predictive value (PPV). 
This problem is equivalent to the one of predicting contacts in a protein structure 

\DIFdelbegin \DIFdel{Figure }\DIFdelend \DIFaddbegin \DIFadd{Fig.~}\DIFaddend $\ref{fig:PPV_L4}$ shows the PPV as a function of the number of predictions for different values of $\gamma$ and $L=4$ (see Fig. \sref{fig:PPV_L10} for the $L=10$ case). 
In this case, the coupling matrix only has $6$ independent non-diagonal elements, and only 6 predictions can be made. 
Our correction systematically outperforms the predictions from the empirical coupling matrix, with an always larger PPV. 
This gain is negligible in the extreme regimes of very high $\gamma$, where the prediction is close to identical to the one obtained with an \emph{i.i.d.} sample, or very low $\gamma$, where it is essentially random. 
It is however much larger in the intermediate regime, with a significantly improved prediction in the region $\gamma/\gamma_d\in[0.5,1]$.


\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/PPVs_L4.png}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/PPVs_L4.pdf}
	\DIFaddendFL \caption{Quality of prediction of interactions for  different values of $\gamma$ and system size $L=4$. Interactions are defined as non-zero elements of the coupling matrix. In the $L=4$ case, there are $6$ possible interactions. Predictions are made by taking the largest elements (in absolute terms) of the inferred coupling matrix. The PPV is the fraction of correctly predicted contacts for a given number of predictions.}
	\label{fig:PPV_L4}
\end{figure}


  



\section{Discussion}
\label{sec:discussion}

In this work, we proposed a method for inferring parameters of an Ornstein-Uhlenbeck process using data that is correlated through an evolutionary tree. 
We kept a very general setting in which data can in principle represent any set of continuous phenotypic traits or potentially discrete sequences if a continuous approximation is made. 
As such, our approach is \DIFdelbegin \DIFdel{only }\DIFdelend \DIFaddbegin \DIFadd{purely }\DIFaddend methodological, and does not directly investigate any particular application. 

We showed that due to the Gaussian and time reversible nature of the OU process, it is possible to write the joint covariance matrix of all data vectors in a simple way. 
The resulting matrix $\mathbb{G}$ consists of block entries that represent covariances between pairs of leaves.
The dependence of these blocks on the coupling matrix $\bm{J}$ characterizing the OU process and on the tree structure can be written explicitly. 
Interestingly, $\mathbb{G}$ only depends on the tree structure through the pairwise \DIFdelbegin \DIFdel{branch }\DIFdelend \DIFaddbegin \DIFadd{path }\DIFaddend length $\Delta t_{ij}$ separating leaves \DIFaddbegin \DIFadd{along the tree}\DIFaddend .

We then proposed a way to compute the likelihood of the data given the tree and the parameters of the OU process, namely the coupling matrix $\mathbf{J}$ and timescale $\gamma$.
This method relies on computing the eigenvalues and vectors of the joint covariance matrix in an efficient manner. 
Indeed, it is possible to separate this calculation in two steps: the first in which we perform the eigen-decomposition of the matrix $\mathbf{J}$, and the second in which we compute eigenvalues and vectors of matrices $\mathbf{G}^a$ that embed the tree structure. 
This reduces the computational complexity from $\mathcal{O}(L^3N^3)$ for a naive inversion of $\mathbb{G}$ to $\mathcal{O}(L^3) + \mathcal{O}(LN^3)$. 
We also show that this method can be used to compute the gradient of the likelihood with respect to parameters with the same complexity.
This makes the problem of inferring $\mathbf{J}$ amenable to maximum likelihood methods using a gradient ascent approach. 

Finally, we showed that this process gives \DIFdelbegin \DIFdel{expected }\DIFdelend \DIFaddbegin \DIFadd{encouraging }\DIFaddend results on simulated data, with a more accurate reconstruction of parameters than if empirical estimation was performed. 
These simulations highlight the fact that this method is only useful in the intermediate regime of phylogenetic correlations. 
If \DIFaddbegin \DIFadd{the timescale }\DIFaddend $\gamma$ \DIFdelbegin \DIFdel{is too high}\DIFdelend \DIFaddbegin \DIFadd{characterizing the branch lengths of the tree is too large}\DIFaddend , correlation of data points through the tree is weak and an empirical estimation performs well. 
On the other hand, a very low $\gamma$ results in strong phylogenetic biases that make recovering $\mathbf{J}$ impossible\DIFaddbegin \DIFadd{, basically due to a strong reduction of the information in a too redundant dataset}\DIFaddend . However, in \DIFdelbegin \DIFdel{the intermediate regime , our }\DIFdelend \DIFaddbegin \DIFadd{an intermediate regime where intrinsic and historical correlations in the dataset coexist, our tree-aware }\DIFaddend re-construction of $\mathbf{J}$ \DIFdelbegin \DIFdel{that takes the tree into account }\DIFdelend results in clear benefits over \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a tree-unaware }\DIFaddend empirical estimation. 

A limitation of our approach remains the long computational time. 
Even with the efficient computation of the gradient, it was necessary to use small system sizes, $L=10$ at most, to repeat our inference process many times with simulated data in a reasonable time. 
For this reason, the framework proposed here is limited to a small number of variables. 
In this respect, it is interesting to note that a different manner of computing the likelihood developed in \cite{mitov_fast_2020} and based on Gaussian integrations on every branch of the tree results in an asymptotic complexity of $\mathcal{O}(NL^3)$. 

Although our method can in principle be used for any set of traits, a major motivation in developing it is its potential application to model of proteins sequences. 
Several results in the last years have shown that selection forces shaping the evolution of protein sequences are well described by a pairwise potential. 
The estimation of this potential is performed using homologous sequences, and is therefore biased by the phylogenetic relations between these sequences.  
Results presented here are a first step in disentangling effects due to phylogeny from effects due to selection in a principled way. 

However, there remain several challenges in using this framework for protein sequences. 
First, the computational power required to process actual sequences is much larger than what was needed for the small simulated systems presented here. 
As an example, a protein of length $L=100$ will be represented by $q\times 100=2000$ Gaussian variables, where $q=20$ is the number of amino acids. 
This is of course much larger than the $L=10$ system used as an example to test our approach. 

A second question is the capacity of a continuous variable approximation, necessary when using Ornstein-Uhlenbeck dynamics, to represent dynamical properties of the landscape protein sequences evolve in. 
This type of approximation has been successfully used before, but in quite different contexts \cite{jones_psicov_2012,barton_large_2014,baldassi_fast_2014}. 
Its use in the context of modelling the evolutionary dynamics of protein sequences remains an open question. 


\textbf{Acknowledgments:} \DIFaddbegin \DIFadd{We acknowledge interesting discussions with Roberto Mulet. PBC and MW acknowlege the hospitality of the Department of  Theoretical  Physics of University  of  Havana, where part of this work was done. Our work was partially funded by the EU H2020 Research and Innovation Programme MSCA-RISE-2016 under Grant
Agreement No. 734439 InferNet.
}\DIFaddend 


\clearpage
\bibliography{bib_phylo}

\bibliographystyle{unsrt}

\newpage
\appendix
\setcounter{figure}{0}
\renewcommand{\figurename}{Figure S}
\setcounter{table}{0}
\renewcommand{\tablename}{Table S}

\section{Supplementary material} % (fold)
\label{sec:supplementary_material}


\subsection{Generating artificial data} % (fold)
\label{sub:generating_artificial_data}

	We are interested in the case where the described Ornstein-Uhlenbeck process takes place on a tree. 
	For example, if configurations $\vec{x}$ represent quantitative traits of some organisms, the tree can represent the genealogy or phylogeny of these organisms. 
	Therefore, we have to be able to simulate the OU process on a tree. 
	In practice, given a rooted tree such as the one shown in figure~\ref{fig:sample_tree} of the main text, we want to sample a configuration $\vec{x}$ for every node in such a way that equation~\eqref{eq:OUjoint} holds for every pair of nodes, the time $\Delta t$ then being the branch length connecting them. \\
	We use a simple methodology to achieve this. 
	First, note that given an arbitrary configuration $\vec{x}_0$ and a time $\Delta t$, we can generate a new configuration $\vec{x}$ distributed according to the propagator in equation~\eqref{eq:OUpropagator} by the transformation
	\begin{equation}
		\vx = \Lam^{\Delta t}\vx_0 + \Sig^{1/2}\vec{\eta}
		\label{eq:OUsample}
	\end{equation}
	where $\Lam$ and $\Sig$ are defined in equation~\eqref{eq:def_lambda}, and $\vec{\eta}$ is a vector of uncorrelated variables with distributions $\mathcal{N}(0,1)$. 
	Moreover, if $\vec{x_0}$ is distributed according to equation~\eqref{eq:eq_distribution}, then $\vx$ and $\vx_0$ are distributed according to equation~\eqref{eq:OUjoint}. 
	Note that equation \eqref{eq:OUsample} is quite different from the Langevin equation~\eqref{eq:langevin}, even though they have similar forms. 
	While the Langevin equation describes the motion of $\vx$ in the potential $\bm{J}$, eq.~\eqref{eq:OUsample} directly samples from the OU process. \\
	Given an already sampled internal node in the tree, equation~\eqref{eq:OUsample} allows to sample a configuration for each of its children. 
	To sample the whole tree, we first sample the root note $\vx_0$ from the equilibrium distribution~\eqref{eq:eq_distribution}. 
	By recursive applications of~\eqref{eq:OUsample}, we then simply work our way down the tree until all leaves are sampled.  


 % subsection generating_artificial_data (end)

\subsection{Initializing parameters} % (fold)
\label{sub:initializing_parameters}

	\subsubsection{Eigenvalues and eigenvectors of $\mathbf{C}^{-1}$} % (fold)
	\label{ssub:eigenvalues_and_eigenvectors}

	The initial value that we take for the covariance matrix is the empirical one
	\begin{equation*}
		\mathbf{C}^{emp} = \frac{1}{N}\sum_{i=1}^{N}\vx_i \cdot\vx_i^{\,T}.
	\end{equation*}
	Its eigenmodes $\{\rho^0_a, \vsa^{\,0}\}$ determine the starting point of the optimization. 

	[Insert here how we go from $\vsa^{\,0}$ to the corresponding set of Eulerian angles or to the corresponding skew symmetric matrix.]

	% From empirical covariance matrix $c^*=\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n \mathbf{x}_n^T$ we could extract information  to  initialize the parameters in the optimization process. 

	% The eigenvalues of empirical couplings $j^*=1/c^*$  will define $\rho^0_i$ ands its eigenvectors  an orthogonal matrix $\bm S^0$ which satisfy the equations systems:
	% \begin{equation}
	% S_{ij}(\theta^0)=S^0_{ij}
	% \label{recurrent_eq}
	% \end{equation}
	% to find the orthogonal parameters which reproduce the initial orthogonal matrix, equation \ref{recurrent_eq} must be inverted or equivalently the solution set $\bm \theta^0$ could be numerically obtained minimizing the function:

	% \begin{equation}
	% f(\bm \theta^0)=\sum_{i,j}\left[ S^0_{ij}-S_{ij}(\bm \theta^0)\right] ^2
	% \label{obtain_theta0}
	% \end{equation}

	% subsubsection eigenvalues_and_eigenvectors (end)

	\subsubsection{Time scale parameter $\gamma$}

	The optimization also requires that we initialize the time scale $\gamma$. 
	For this, we try to find the optimal $\gamma$ given the data $\mathbf{X}$, the tree, and the OU process defined by the empirical covariance matrix. 

	The probability distribution $P$ for the configurations of two leaves $\vx_i$ and  $\vx_j$  separated by time $\Delta t_{ij}$ is given by equation~\eqref{eq:OUjoint} of the main text. 
	With this distribution we can calculate analytically the average of the scalar product $\vx_i^{\,T}\vx_j$:
	\begin{align*}
		\langle \vx_i^{\,T}\vx_j \rangle_P &= \int \ddroit \vx_i \ddroit \vx_j P(\vx_i,\vx_j|\Delta t_{ij}) \sum_{a=1}^L   x_i^{a}x_j^{a}  \\
		&=\sum_{i=a}^L \langle  x_i^{a} x_j^a \rangle_{P}.\\
	\end{align*}
	The covariance $\langle  x_i^{a} x_j^a \rangle_{P}$ between observations separated by a time $\Delta t_{ij}$ is given by equation~\eqref{eq:pairwisecov} of the main text.
	Using this, we now have
	\begin{align}
		\nonumber\langle \vx_i^{\,T}\vx_j \rangle_P &= \sum_{a=1}^L \left( \Lam^{\Delta t_{ij}} \mathbf{C} \right)_{aa}\\
		\nonumber&= \Tr\left( \Lam^{\Delta t_{ij}} \mathbf{C} \right)\\
		&= \sum_{a=1}^L \rho_a^{-1}e^{-\gamma\rho_a\Delta t_{ij}}.
		\label{eq:scalar_product}
	\end{align}
	Having initialized the covariance matrix $\mathbf{C}$ at its empirical value, we know the values of all members of the r.h.s. of equation~\eqref{eq:scalar_product} except that of $\gamma$. 
	On the other hand, equivalent versions of equation~\eqref{eq:scalar_product} can be written for all pairs of configurations $i$ and $j$. 
	To find an initial value of $\gamma$ which is consistent with the data and the empirical covariance matrix, we search for one that best explains the observed scalar products between configurations. 
	We thus define $\gamma^0$ to be the argument minimizing the functional $F(\gamma)$: 
	\begin{equation}
	 	F(\gamma) = \sum_{1\leq i<j\leq N}\left[ \vx_i^{\,T}\vx_j - \sum_{a=1}^L \rho_a^{-1}e^{-\gamma\rho_a\Delta t_{ij}} \right].
	\end{equation} 
	As $F$ depends on one scalar parameter, it is straightforward to minimize it, allowing us to initialize $\gamma$ to a reasonable value.

% subsection initializing_parameters (end)


% section supplementary_material (end)

\subsection{Parametrizations of  eigenvectors } % (fold)
\label{sub:parametrizations_of_eigenvectors}
\subsubsection{Parametrization in term of generalized Eulerian angles}
The aim is to  parameterize each base vector $\vec{s}_a$ according to the $a-th$ column of an arbitrary orthogonal matrix $\bm S$ parameterized   in terms of $L(L-1)/2$ independent variables  $\theta_ {pq}, p =1,2, ..., L; q =1,2, ..., L; p <q $, named Generalized Eulerian angles (Eulerian angles is for L=3).

To construct this matrix we start from a  transformation in a two-dimensional subspace of an $L-dimensional$ space which is given by an L-dimensional matrix of the  form:

\begin{equation} 
\bm T_{pq} =  \left(
\begin{array}{cccccc}
1 &   &  &  &  &\\
 & 1& & & & \\
& & \cos\theta_{pq} & & \sin\theta_{pq}\\
& &  & 1 & &\\
& &  -\sin\theta_{pq}& & \cos\theta_{pq} &\\
& &  & &  & 1\\
\end{array}
\right)
\end{equation}
where all diagonal elements are unity except the diagonal elements in the $pth$ column and the $qth$ column, which are $\cos\theta_{pq}$ and all off-diagonal elements are zero except the one corresponding to the intersection of the $pth$ row and the $qth$ column, which is $\sin\theta_{pq}$, and that on the intersection of the $qth$ row and the $pth$ column, which is $-\sin\theta_{pq}$. There are $L(L - 1)/2$  matrices of the form indicated in Equation (1), corresponding to all choices of $p$ and $q$ with $p <q$.

An arbitrary  $L-dimensional$ orthogonal matrix $\bm S$ can be represented as a product of these $L(L-1)/2$ orthogonal matrices with appropriate values of the $L(L-1)/2$ independent parameters $\theta_{pq}$.

\begin{equation}
\bm S (\bm \theta)=\prod_{\left\lbrace pq\right\rbrace }T_{pq}(\theta_{pq})
\end{equation}

 This matrix is in charge of rotations of vectors in $L-dimensional$ space. For $L=3$ we get the classical Eulerian matrix. 

 In reference [7] is explained an recursive algorithm to efficiently perform this multiplication,  as well as the   construction of the matrix   derivatives respect to parameters $\theta_{pq}$. 

For the first, equation  (2) is transformed in  the recurrence equations:

\begin{equation}
\bm S(\vec{\theta} )=\bm T^{(L)}
\end{equation}

with $\theta_ {pq}, p =1,2, ..., L; q =1,2, ..., L; p <q$ and 


\begin{equation}
T^{(n)}_{kl}=\cos\theta_{kn} * t^{(n)}_{kl}-\sin\theta_{kn}* s^{(n)}_{kl}
\end{equation}
with
\begin{equation}
\begin{split}
&s^{(n)}_{k+1,l}=\sin\theta_{kn} * t^{(n)}_{kl}+\cos\theta_{kn}*s^{(n)}_{kl}\\
&s^{(n)}_{k+1,n}=\cos\theta_{kn}*s^{(n)}_{kn}\\
&s^{(n)}_{1,l}=-\delta_{ln}\\
&\theta_{nn}=\pi/2\\
&\end{split}
\end{equation}
and
\begin{equation} 
\bm t^{(n)} =  \left(
\begin{array}{cc}
\bm T^{(n-1)} & \bm 0  \\
\bm 0 &  \bm I\\

\end{array}
\right)
\end{equation}

This mean that the eigenvectors $\vec{s}_a$ of the matrix $\bm C$ with dimensions $L\times L$ can be written as:
\begin{equation}
s^{k}_a=T^{(L)}_{.,a}=\cos\theta_{kL} * t^{(L)}_{ka}-\sin\theta_{kL}* s^{(L)}_{ka} \;\;\; \text{for} \;k=1,..,L
\end{equation}



Then there is another  important problem : how to determine parameters $\bm \theta$  given an orthogonal matrix $\bm Q$ such that all equations 

$$S_{ij}(\bm \theta)=Q_{ij}$$

are satisfied. This system of nonlinear transcendental equations  can't be algebraically solved, however it's possible overcome  this issue finding the set of $\bm \theta$ which minimize the function:   
\begin{equation}
f(\bm \theta)=\sum_{i,j}\left[ Q_{ij}-S_{ij}(\bm \theta)\right] ^2
\end{equation}
This is useful when we initialize parameters $\bm \theta$  from the matrix formed by eigenvectors of the empirical covariance matrix.
\subsubsection{Parametrization in term of the exponential of a skew-symmetric matrix }
The exponential of a skew-symmetric matrix $\bm X=-\bm X^T$  : 
 \begin{equation}
 \bm S=\exp (\bm X)
 \end{equation}

 is  an  orthogonal  matrix : $\exp(X)^T=\exp(X^T)=\exp(-X)=\exp(X)^{-1}$. 

 The derivatives on $L(L-1)/2$ independent variables of $\bm X$  is formally defined by

 \begin{equation}
 \frac{\partial \bm S}{\partial X_{jk}}=\lim_{h\rightarrow 0} \frac{1}{h} \left( \exp(\bm X+ h \bm E^{jk}) - \exp(\bm X)\right) 
 \end{equation}

 where $\bm E^{jk}$ for $j > k$ is defined as a skew-symmetric matrix that has only two nonzero elements:
 \begin{equation}
 E^{pq}_{jk}=\delta_{pj}\delta_{qk}-\delta_{pk}\delta_{qj}
 \end{equation}

To obtain   a skew-symmetric matrix    $\bm X$ from an orthogonal matrix $\bm Q$    is enough invert the exponential relation :
 $$\bm X=\log \bm Q$$

  If we write eigenvectors of the likelihood function as the columns of the exponential of the skew-symmetric matrix $\bm X$, then we are able to perform the optimization over it's $L(L-1)$ independent variables of $\bm X$.

 Furthermore, as the eigenvectors were parameterized by an algebraic function, the complete likelihood  is described in terms of arithmetics operation and elementary functions. This allow to compute  the gradient via  automatic differentiation (AD), making computations in our optimization process faster. 
 The idea  is to represent a function as a computational graph, also called a Wengert list , where each node  in this list will represent an intermediate result of the computation. The intermediate results can then be assembled using the chain rule to get the final derivative we’re looking for. There is two    main algorithms  to traverses the chain rule in AD: forward mode and reverse mode, here we used reverse mode  algorithm  which is the of choice for back-propagation in deep learning . In particular we implemented  it using the Julia package Zygote.jl . 

 \subsection{Homogeneous and fully balanced tree}
 \label{sub:homogeneous_and_fully_balanced_tree}

Let's assume that the tree is binary, symmetric and completely homogeneous with all branches having the same length $\Delta t$. 
As an example, the covariance matrix for such a tree with $K=2$ branching events and four leaves is
\begin{equation} 
\mathbb{G} =  \left(
\begin{array}{cccc}
{ \bm C} & {\bm C} \Lambda ^{2\Delta t} & {\bm C} \Lambda ^{4\Delta t} & {\bm C} \Lambda ^{4\Delta t} \\
{\bm C} \Lambda ^{2\Delta t} & {\bm C} & {\bm C} \Lambda ^{4\Delta t} & {\bm C} \Lambda ^{4\Delta t} \\
{\bm C} \Lambda ^{4\Delta t} & {\bm C} \Lambda ^{4\Delta t} & {\bm C} & {\bm C} \Lambda ^{2\Delta t} \\
{\bm C} \Lambda ^{4\Delta t} & {\bm C} \Lambda ^{4\Delta t} & {\bm C} \Lambda ^{2\Delta t} & {\bm C} \\
\end{array}\label{eq:G}
\right).
\end{equation}

The associated matrix $\bm G^a=z(\rho_a,\gamma,\Delta t)$  defined in Eq.~\eqref{eq:G} becomes

\begin{equation} 
\bm G^a =  \left(
\begin{array}{cccc}
{  \rho^{-1}_a } & {\rho^{-1}_a } e ^{-2\gamma\rho_a\Delta t} & {\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t}& {e ^{-4\gamma\rho_a\Delta t}} \\
{\rho^{-1}_a } e ^{-2\gamma\rho_a\Delta t} & {\rho^{-1}_a } & {\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t} & {\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t} \\
{\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t} & {\rho^{-1}_a} e ^{-4\gamma\rho_a\Delta t}& {\rho^{-1}_a } & {\rho^{-1}_a } e ^{-2\gamma\rho_a\Delta t}\\
{\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t} & {\rho^{-1}_a } e ^{-4\gamma\rho_a\Delta t} & {\rho^{-1}_a } e ^{-2\gamma\rho_a\Delta t} & {\rho^{-1}_a } \\
\end{array}\label{eq:G_a}
\right).
\end{equation}

For hyper-geometric matrices as \eqref{eq:G_a} of dimension $2^K$, there are $K+1$ different eigenvalues  given by: 

\begin{equation}
\label{eq:lambda_hyper}
\lambda_k(\rho_a,\gamma) =\rho^{-1}_a *
\begin{cases}
 (1+\sum_{l=1}^{k-1} 2^{l-1} e ^{-2l\gamma\rho_a\Delta t}-2^{k-1} e ^{-2k\gamma\rho_a\Delta t})\;\;\; k\in[1,K] \\
 (1+\sum_{l=1}^{K} 2^{l-1} e ^{-2l\gamma\rho_a\Delta t}) \;\;\;\;\;\;\;k=K+1
\end{cases}
\end{equation}

where $\lambda_{K+1}\ge\lambda_{K}\cdots\ge\lambda_{1}$. For $k<K+1$, the degeneracy of eigenvalue $\lambda_k$ is $d_k=2^{K-k}$.  
The associated eigenvectors are independent of the parameter $\rho_a$and reflect the events in the phylogenetic tree.
Each eigenvector $\vec{u}_k$ captures the duplication events in the $(K+1 -k) st$ generation:

\begin{equation} 
\nonumber
 \vec{u}_k =
\begin{cases}
{(\underbrace{\overbrace{1,\ldots,1}^{2^{k-1}},\overbrace{-1,\ldots,-1}^{2^{k-1}}}_Q,0,\ldots,0)\ \bigcup \Gamma(u_k) } \;\;\; k\in[1,K] \\
(1,1,1,\ldots,1,1,1)\;\;\;\;\;\;\;k=K+1
\label{eq:eigenvec_simpler_case}
\end{cases}	
\end{equation}

where $\Gamma(\vec{u}_k )$ represents the $d_k$ combinations obtained by shifting the block of length $Q$, generating all eigenvectors corresponding to the eigenvalue $\lambda_k$.  The eigenvectors are orthogonal to each other, and can be  normalized and arranged horizontally into a matrix $U$. 

To compute the gradient of the likelihood, we need  derivatives of $\lambda_k(\rho_a,\gamma)$ with respect to $\rho_a$ and $\gamma$, which can be directly obtained from expression \eqref{eq:lambda_hyper}.


\subsection{Optimization scheme}
\label{sub:optimization_scheme}

The proposed inference scheme was transformed into a multidimensional nonlinear optimization problem for which a variant  of quasi-Newton Methods (QNMs) was used.  The main feature in QNMs is that Hessian matrix $\hat{H}$  does not need to be computed instead it’s approximated. The Hessian approximation $\hat{H}$ is chosen to satisfy the secant equation:

$$\nabla L(\vec{x}_k+\Delta\vec{x})=\nabla L(\vec{x}_k)+\hat{H} \Delta\vec{x} $$


for $n-dimensions$ $\hat{H}$ is undetermined, the various QNMs differ in their choice of the solution to the secant equation. We used LBFGS variant  (limited memory version of BFGS), this particular method is based in choosing $\hat{H}$ as a positive definite matrix where

$$\hat{H}_{k+1}=\hat{H}_{k}+\frac{y_k y^{T}_k}{y_k\Delta x_k} - \frac{\hat{H}_k\Delta x_k(\hat{H}_k\Delta x_k)^T}{\Delta x^T_k\hat{H}_k\Delta x_k} \;\;\;\text{and}\;\;\; y_k=\nabla L(\vec{x}_k+1)-\nabla L(\vec{x}_k)$$

The method implementation was done using   NLopt package \DIFdelbegin \DIFdel{(see reference) }%DIFDELCMD < \pierre{Add reference?}%%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{NLopt} }\hspace{0pt}%DIFAUXCMD
}\DIFaddend .

\subsection{Supplementary figures}
\label{sub:supplementary_figures}

\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/error_Ciid_vs_numb_of_seq.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/error_Ciid_vs_numb_of_seq.pdf}
	\DIFaddendFL \caption{Relative $l2$-error between the empirical covariance matrix calculated from an \emph{i.i.d.} sample and the true covariance matrix, for system sizes $L=4$ and $L=10$. The dashed vertical line corresponds to the number of leaves of the tree used in the simulations.}
	\label{fig:error_vs_nseqs}
\end{figure}

\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/scatter_gamma_L4_100.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/scatter_gamma_L4_100.pdf}
	\DIFaddendFL \caption{Inferred $\gamma$ values as a function of real $\gamma$, for system size $L=4$. $\gamma_{emp}$ is the value obtained by the process described in section \ref{sub:initializing_parameters} of the SM. $\gamma_{max}$ is the value inferred by the maximum-likelihood calculation. 
	The inset represents the ratio of both inferred parameters $\gamma_{emp}$ or $\gamma_{max}$ to the real $\gamma$.}
	\label{fig:scatter_gamma_L4}
\end{figure}


\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.55\textwidth]{Figures/Pearson_correlation_rate_full_range.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/Pearson_correlation_rate_full_range.pdf}
	\DIFaddendFL \caption{  Ratio between Pearson correlation of the  maximum-likelihood  and true covariance  matrix to the Pearson correlation of the  empirical and true covariances matrices for the two system sizes $L=4$ and $L=10$.}
	\label{fig:pearson_corr_comp_L4}
\end{figure}

\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.65\textwidth]{Figures/corr_C_L10_balanced_tree_100_rep.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/corr_C_L10_balanced_tree_100_rep.pdf}
	\DIFaddendFL \caption{ Pearson correlation between empirical /maximum-likelihood   covariance matrices and the true covariance matrix, the inset plot represent the ratio between the person correlation for the  maximum-likelihood covariance matrix and the one for the empirical covariance matrix.}
	\label{fig:pears_L10}
\end{figure}


\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.65\textwidth]{Figures/scatter_gamma_L10_100.pdf}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=0.5\textwidth]{Figures/scatter_gamma_L10_100.pdf}
	\DIFaddendFL \caption{Inferred $\gamma$ values as a function of real $\gamma$, for system size $L=10$. $\gamma_{emp}$ is the value obtained by the process described in section \ref{sub:initializing_parameters} of the SM. $\gamma_{max}$ is the value inferred by the maximum-likelihood calculation. 
	The inset represents the ratio of both inferred parameters $\gamma_{emp}$ or $\gamma_{max}$ to the real $\gamma$. }
	\label{fig:scatter_gamma_L10}
\end{figure}


 \begin{figure*}[!htb]
 	\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}{}
%DIFDELCMD <  			\centering\includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_C_L10_balanced_tree_100.pdf}
%DIFDELCMD <  		\end{subfigure}
%DIFDELCMD <  		%%%
\DIFdelFL{\hspace{1mm}
 }%DIFDELCMD < \begin{subfigure}{}
%DIFDELCMD <  			%%%
\DIFdelendFL %DIF > 	\begin{subfigure}{}
 			\centering\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_J_L10_balanced_tree_100.pdf}
%DIFDELCMD <  		\end{subfigure}
%DIFDELCMD <  	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/epsilon_error_L10_balanced_tree_100.pdf}
 %DIF > 		\end{subfigure}
 		\DIFaddFL{\hspace{1mm}
 %DIF > 		\begin{subfigure}{}
 %DIF > 			\centering\includegraphics[keepaspectratio=true,width=0.45\textwidth]{Figures/epsilon_error_J_L10_balanced_tree_100.pdf}
 %DIF > 		\end{subfigure}
 	}\DIFaddendFL \caption{\textbf{Left:} Relative $l2$-error between empirical or maximum-likelihood covariance matrices and the true covariance matrix. \textbf{Right}: Relative $l2$-error between empirical /maximum-likelihood coupling matrices and the true coupling matrix. Logarithmic scale is chosen for the y-axis because of large values of the error at low $\gamma$. The inset in both panels show the ratio between the two errors. For system size $L=10$.}
 	\label{fig:error_1_L10}
 \end{figure*}



  
\begin{figure}[!htb]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/PPVs_L10.png}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[keepaspectratio=true,width=1.0\textwidth]{Figures/PPVs_L10.pdf}
	\DIFaddendFL \caption{Quality of prediction of interactions for  different values of $\gamma$ and system size $L=4$. Interactions are defined as non-zero elements of the coupling matrix. In the $L=4$ case, there are $6$ possible interactions. Predictions are made by taking the largest elements (in absolute terms) of the inferred coupling matrix. The PPV is the fraction of correctly predicted contacts for a given number of predictions.}
	\label{fig:PPV_L10}
\end{figure}

\end{document}
